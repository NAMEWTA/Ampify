# Model Proxy æ¨¡å—

## æ¨¡å—æ¦‚è¿°
Model Proxy åˆ©ç”¨ VS Code `vscode.lm` API è·å– Copilot Chat å¯ç”¨æ¨¡å‹ï¼Œåœ¨æœ¬åœ°å¯åŠ¨ HTTP åä»£ç†æœåŠ¡å™¨ï¼ŒåŒæ—¶æš´éœ² **OpenAI** å…¼å®¹è·¯ç”± (`/v1/chat/completions`) å’Œ **Anthropic** å…¼å®¹è·¯ç”± (`/v1/messages`)ã€‚å¤–éƒ¨åº”ç”¨é€šè¿‡ `BaseUrl + API Key` å³å¯ç›´æ¥è°ƒç”¨ VS Code å†…çš„ LLM èƒ½åŠ›ã€‚

### æ ¸å¿ƒèƒ½åŠ›
- ä¸€é”®å¼€å¯/å…³é—­æœ¬åœ°ä»£ç†ï¼ˆMainView + çŠ¶æ€æ  + å‘½ä»¤ï¼‰
- OpenAI & Anthropic åŒæ ¼å¼å…¼å®¹ï¼ˆå•ç«¯å£åŒè·¯ç”±ï¼‰
- è‡ªåŠ¨ç”Ÿæˆ API Keyï¼Œæ”¯æŒæŸ¥çœ‹/å¤åˆ¶/é‡æ–°ç”Ÿæˆ
- æµå¼ (SSE) ä¸éæµå¼å“åº”
- è‡ªåŠ¨æ¨¡å‹å‘ç°ä¸è·¯ç”±
- JSONL æŒ‰æ—¥æœŸåˆ†å‰²è¯·æ±‚æ—¥å¿—

## ç›®å½•ç»“æ„
```
src/modules/modelProxy/
â”œâ”€â”€ index.ts                        # registerModelProxy(context) å…¥å£
â””â”€â”€ core/
    â”œâ”€â”€ proxyConfigManager.ts       # é…ç½®ç®¡ç†ï¼ˆå•ä¾‹ï¼‰
    â”œâ”€â”€ proxyServer.ts              # HTTP æœåŠ¡å™¨ç”Ÿå‘½å‘¨æœŸ
    â”œâ”€â”€ modelBridge.ts              # VS Code LM â†” å¤–éƒ¨ API æ¶ˆæ¯è½¬æ¢
    â”œâ”€â”€ openaiHandler.ts            # OpenAI æ ¼å¼è¯·æ±‚/å“åº”å¤„ç†
    â”œâ”€â”€ anthropicHandler.ts         # Anthropic æ ¼å¼è¯·æ±‚/å“åº”å¤„ç†
    â”œâ”€â”€ authManager.ts              # API Key ç”Ÿæˆ/æå–/éªŒè¯
    â””â”€â”€ logManager.ts               # JSONL æ—¥å¿—ç®¡ç†
```

## æ ¸å¿ƒç±»ä¸èŒè´£

| ç±» | èŒè´£ |
|---|---|
| `ProxyConfigManager` | å•ä¾‹ã€‚ç®¡ç† `modelproxy/config.json`ï¼ˆç«¯å£ã€Keyã€å¯ç”¨çŠ¶æ€ã€é»˜è®¤æ¨¡å‹ã€æ—¥å¿—å¼€å…³ï¼‰ã€‚ç«¯å£/ç»‘å®šåœ°å€ä¼˜å…ˆè¯»å– VS Code Settings |
| `AuthManager` | é™æ€å·¥å…·ç±»ã€‚`generateKey()` ç”Ÿæˆ `amp-<64hex>` æ ¼å¼ Keyï¼›`extractKey()` ä» `Authorization: Bearer` æˆ– `x-api-key` æå–ï¼›`validateRequest()` ä½¿ç”¨ `timingSafeEqual` é˜²æ—¶åºæ”»å‡» |
| `ModelBridge` | ç®¡ç†æ¨¡å‹ç¼“å­˜ï¼ˆ`Map<id, LanguageModelChat>`ï¼‰ã€æ¨¡å‹æŸ¥æ‰¾ï¼ˆç²¾ç¡®â†’familyâ†’æ¨¡ç³Šï¼‰ã€æ¶ˆæ¯æ ¼å¼è½¬æ¢ã€token ä¼°ç®—ã€‚ç›‘å¬ `onDidChangeChatModels` è‡ªåŠ¨åˆ·æ–° |
| `OpenAIHandler` | å¤„ç† `POST /v1/chat/completions`ï¼ˆæµå¼/éæµå¼ï¼‰å’Œ `GET /v1/models`ã€‚æµå¼è¾“å‡º SSE `data: {...}\n\n` + `data: [DONE]` |
| `AnthropicHandler` | å¤„ç† `POST /v1/messages`ã€‚æµå¼è¾“å‡º `event: message_start` â†’ `content_block_delta` â†’ `message_stop` |
| `ProxyServer` | Node.js `http.createServer`ã€‚è·¯ç”±åˆ†å‘ã€CORSã€è®¤è¯ä¸­é—´ä»¶ã€è¯·æ±‚ä½“é™åˆ¶ (1MB)ã€`/health` ç«¯ç‚¹ |
| `LogManager` | JSONL æ—¥å¿—å†™å…¥ `~/.vscode-ampify/modelproxy/logs/YYYY-MM-DD.jsonl`ã€‚æä¾› `getRecentLogs()` å’Œ `getTodayStats()` |

## VS Code API ä¾èµ–

| API | ç”¨é€” |
|---|---|
| `vscode.lm.selectChatModels(selector?)` | æšä¸¾å¯ç”¨æ¨¡å‹åˆ—è¡¨ |
| `vscode.lm.onDidChangeChatModels` | ç›‘å¬æ¨¡å‹å˜åŒ–ï¼Œåˆ·æ–°ç¼“å­˜ |
| `LanguageModelChat.sendRequest(messages, options, token)` | å‘æ¨¡å‹å‘é€è¯·æ±‚ |
| `LanguageModelChatResponse.stream` / `.text` | æµå¼è¯»å–å“åº” |
| `LanguageModelChatMessage.User()` / `.Assistant()` | æ„é€ æ¶ˆæ¯ |
| `LanguageModelChat.countTokens()` | token ä¼°ç®— |
| `LanguageModelChatRequestOptions.modelOptions` | é€ä¼  temperature ç­‰å‚æ•° |
| `vscode.CancellationTokenSource` | å®¢æˆ·ç«¯æ–­å¼€æ—¶å–æ¶ˆè¯·æ±‚ |
| `vscode.LanguageModelError` | æ•è·æƒé™/æ‰¾ä¸åˆ°æ¨¡å‹ç­‰é”™è¯¯ |

## ä¸šåŠ¡æµç¨‹

### ä»£ç†è¯·æ±‚æ•°æ®æµ

```mermaid
sequenceDiagram
    participant Client as å¤–éƒ¨åº”ç”¨
    participant Server as ProxyServer (HTTP)
    participant Auth as AuthManager
    participant Router as è·¯ç”±åˆ†å‘
    participant Handler as OpenAI/Anthropic Handler
    participant Bridge as ModelBridge
    participant LM as vscode.lm (Chat Model)
    participant Log as LogManager

    Client->>Server: POST /v1/chat/completions
    Server->>Auth: validateRequest(req, expectedKey)
    Auth-->>Server: âœ“ valid
    Server->>Router: pathname â†’ OpenAI
    Router->>Handler: handleChatCompletions(body, res)
    Handler->>Bridge: findModel(request.model)
    Bridge-->>Handler: LanguageModelChat
    Handler->>Bridge: convertOpenAIMessages(messages)
    Bridge-->>Handler: LanguageModelChatMessage[]
    Handler->>LM: model.sendRequest(messages, options, token)
    LM-->>Handler: LanguageModelChatResponse (stream)
    
    alt stream = true
        loop for each text chunk
            Handler->>Client: SSE data: {"choices":[{"delta":{"content":"..."}}]}
        end
        Handler->>Client: data: [DONE]
    else stream = false
        Handler->>Handler: collect full text
        Handler->>Client: JSON {"choices":[{"message":{"content":"..."}}]}
    end

    Handler->>Log: log(entry)
```

### ä»£ç†å¯åŠ¨/å…³é—­æµç¨‹

```mermaid
sequenceDiagram
    participant UI as MainView / å‘½ä»¤
    participant Mod as index.ts
    participant CM as ProxyConfigManager
    participant MB as ModelBridge
    participant PS as ProxyServer

    UI->>Mod: ampify.modelProxy.toggle
    Mod->>CM: getConfig()
    
    alt å½“å‰æœªè¿è¡Œ
        Mod->>MB: refreshModels()
        MB-->>Mod: models ready
        Mod->>PS: start(port, bindAddress)
        PS-->>Mod: listening
        Mod->>CM: saveConfig({ enabled: true })
        Mod->>UI: çŠ¶æ€æ æ›´æ–° "Proxy :18080"
    else å½“å‰è¿è¡Œä¸­
        Mod->>PS: stop()
        PS-->>Mod: closed
        Mod->>CM: saveConfig({ enabled: false })
        Mod->>UI: çŠ¶æ€æ æ›´æ–° "Proxy Off"
    end
```

## HTTP è·¯ç”±è¡¨

| æ–¹æ³• | è·¯å¾„ | è®¤è¯ | å¤„ç†å™¨ | è¯´æ˜ |
|---|---|---|---|---|
| `GET` | `/health` | å¦ | ProxyServer | å¥åº·æ£€æŸ¥ |
| `GET` | `/v1/models` | æ˜¯ | OpenAIHandler | æ¨¡å‹åˆ—è¡¨ï¼ˆOpenAI æ ¼å¼ï¼‰ |
| `POST` | `/v1/chat/completions` | æ˜¯ | OpenAIHandler | èŠå¤©è¡¥å…¨ï¼ˆæ”¯æŒ streamï¼‰ |
| `POST` | `/v1/messages` | æ˜¯ | AnthropicHandler | æ¶ˆæ¯ APIï¼ˆæ”¯æŒ streamï¼‰ |
| `OPTIONS` | `*` | å¦ | ProxyServer | CORS é¢„æ£€ |

## è®¤è¯æ–¹å¼
æ”¯æŒä¸¤ç§å¤´éƒ¨ï¼ˆè‡ªåŠ¨è¯†åˆ«ï¼‰ï¼š
- `Authorization: Bearer amp-xxxx...` (OpenAI é£æ ¼)
- `x-api-key: amp-xxxx...` (Anthropic é£æ ¼)

Key æ ¼å¼ï¼š`amp-` + 64 ä½éšæœº hexï¼ˆ`crypto.randomBytes(32)`ï¼‰

## æ¨¡å‹åŒ¹é…ç­–ç•¥
å¤–éƒ¨è¯·æ±‚çš„ `model` å­—æ®µæŒ‰ä¼˜å…ˆçº§åŒ¹é…ï¼š
1. **ç²¾ç¡®åŒ¹é…** `model.id`
2. **family åŒ¹é…** `model.family`ï¼ˆå¦‚ `gpt-4o`ï¼‰
3. **æ¨¡ç³ŠåŒ¹é…** id/family/name åŒ…å«è¯·æ±‚å­—ç¬¦ä¸²
4. **å…œåº•** ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹

## æ¶ˆæ¯æ ¼å¼è½¬æ¢

### OpenAI â†’ VS Code
| OpenAI role | VS Code |
|---|---|
| `system` | `LanguageModelChatMessage.User()` |
| `user` | `LanguageModelChatMessage.User()` |
| `assistant` | `LanguageModelChatMessage.Assistant()` |
| `tool` | `LanguageModelChatMessage.User()` |

### Anthropic â†’ VS Code
- `system` å‚æ•° â†’ ç¬¬ä¸€æ¡ `User` æ¶ˆæ¯
- `user` / `assistant` â†’ å¯¹åº” `User()` / `Assistant()`
- Content blocks å±•å¹³ä¸ºçº¯æ–‡æœ¬

## å‘½ä»¤æ³¨å†Œ

| å‘½ä»¤ ID | è¯´æ˜ |
|---|---|
| `ampify.modelProxy.toggle` | åˆ‡æ¢ä»£ç†å¼€/å…³ |
| `ampify.modelProxy.start` | å¯åŠ¨ä»£ç† |
| `ampify.modelProxy.stop` | åœæ­¢ä»£ç† |
| `ampify.modelProxy.copyKey` | å¤åˆ¶ API Key |
| `ampify.modelProxy.regenerateKey` | é‡æ–°ç”Ÿæˆ Key |
| `ampify.modelProxy.copyBaseUrl` | å¤åˆ¶ Base URL |
| `ampify.modelProxy.selectModel` | é€‰æ‹©é»˜è®¤æ¨¡å‹ï¼ˆQuickPickï¼‰ |
| `ampify.modelProxy.viewLogs` | æ‰“å¼€æ—¥å¿—æ–‡ä»¶å¤¹ |
| `ampify.modelProxy.refresh` | åˆ·æ–°æ¨¡å‹åˆ—è¡¨ |

## VS Code Settings

| é…ç½®é¡¹ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|---|---|---|---|
| `ampify.modelProxy.port` | `number` | `18080` | HTTP æœåŠ¡ç«¯å£ |
| `ampify.modelProxy.bindAddress` | `string` | `127.0.0.1` | ç»‘å®šåœ°å€ |

> `apiKey`ã€`enabled`ã€`defaultModelId`ã€`logEnabled` å­˜å‚¨åœ¨æ¨¡å—æœ¬åœ° `config.json`ï¼Œè€Œé VS Code Settingsï¼ˆæ•æ„Ÿæ•°æ®/è¿è¡Œæ—¶çŠ¶æ€ï¼‰ã€‚

## æ•°æ®å­˜å‚¨

```
~/.vscode-ampify/
â””â”€â”€ modelproxy/
    â”œâ”€â”€ config.json          # ProxyConfig
    â””â”€â”€ logs/
        â”œâ”€â”€ 2026-02-06.jsonl
        â””â”€â”€ ...
```

### ProxyConfig ç»“æ„
```json
{
  "port": 18080,
  "apiKey": "amp-a1b2c3d4...",
  "enabled": false,
  "defaultModelId": "",
  "logEnabled": true,
  "bindAddress": "127.0.0.1"
}
```

### æ—¥å¿—æ¡ç›®ï¼ˆJSONL æ¯è¡Œï¼‰
```json
{
  "timestamp": "2026-02-06T14:23:05.123Z",
  "requestId": "550e8400-e29b-41d4-a716-446655440000",
  "format": "openai",
  "model": "gpt-4o",
  "inputTokens": 128,
  "outputTokens": 256,
  "durationMs": 1234,
  "status": "success"
}
```

## MainView Bridge

### TreeNode ç»“æ„
```
ğŸ“¡ Model Proxy: è¿è¡Œä¸­ :18080        [Start/Stop]
ğŸ”— åœ°å€: http://127.0.0.1:18080       [Copy]
ğŸ”‘ API Key: amp-a1b2...xxxx           [Copy] [Regenerate]
ğŸ¤– é»˜è®¤æ¨¡å‹: GPT-4o (2 available)     [Change]
ğŸ“Š ä»Šæ—¥ç»Ÿè®¡: 5 æ¬¡è¯·æ±‚ Â· 3200 tokens
ğŸ“‹ æœ€è¿‘æ—¥å¿— (å¯å±•å¼€)
  â”œâ”€â”€ 14:23:05  openai gpt-4o 1.2s âœ“
  â”œâ”€â”€ 14:22:58  anthropic claude-3 0.8s âœ“
  â””â”€â”€ 14:22:31  openai gpt-4o 2.1s âœ—
```

### Toolbar
- Toggle (Start/Stop)
- Refresh
- Open Logs

## å¤–éƒ¨è°ƒç”¨ç¤ºä¾‹

### OpenAI æ ¼å¼
```bash
curl http://127.0.0.1:18080/v1/chat/completions \
  -H "Authorization: Bearer amp-xxxxx" \
  -H "Content-Type: application/json" \
  -d '{"model":"gpt-4o","messages":[{"role":"user","content":"Hello"}],"stream":true}'
```

### Anthropic æ ¼å¼
```bash
curl http://127.0.0.1:18080/v1/messages \
  -H "x-api-key: amp-xxxxx" \
  -H "Content-Type: application/json" \
  -d '{"model":"claude-sonnet-4-20250514","messages":[{"role":"user","content":"Hello"}],"max_tokens":1024}'
```

### æ¨¡å‹åˆ—è¡¨
```bash
curl http://127.0.0.1:18080/v1/models \
  -H "Authorization: Bearer amp-xxxxx"
```

## å®‰å…¨æ³¨æ„äº‹é¡¹
- é»˜è®¤ä»…ç»‘å®š `127.0.0.1`ï¼Œå¤–éƒ¨è®¿é—®éœ€æ˜¾å¼é…ç½® `0.0.0.0`
- API Key ä½¿ç”¨ `crypto.timingSafeEqual` éªŒè¯ï¼Œé˜²æ­¢æ—¶åºæ”»å‡»
- è¯·æ±‚ä½“é™åˆ¶ 1MB
- å®¢æˆ·ç«¯æ–­å¼€æ—¶é€šè¿‡ `CancellationToken` å–æ¶ˆ VS Code è¯·æ±‚
- Key å­˜å‚¨åœ¨æœ¬åœ° `config.json`ï¼Œä¸è¿›å…¥ VS Code Settings åŒæ­¥

## å…³é”®çº¦æŸ
- `LanguageModelChat.sendRequest()` é¦–æ¬¡è°ƒç”¨æ—¶ VS Code ä¼šå¼¹å‡ºç”¨æˆ·æˆæƒå¯¹è¯æ¡†
- ä½¿ç”¨ Node.js å†…ç½® `http` æ¨¡å—ï¼Œæ— éœ€é¢å¤–ä¾èµ–
- `deactivate()` æ—¶é€šè¿‡ context subscriptions è‡ªåŠ¨è°ƒç”¨ `proxyServer.stop()` å’Œ `modelBridge.dispose()`
